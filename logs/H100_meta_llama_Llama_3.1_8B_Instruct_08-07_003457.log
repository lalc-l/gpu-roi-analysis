2025-08-07 00:34:57,901 - INFO - Starting benchmark [PRODUCTION]: meta-llama/Llama-3.1-8B-Instruct on H100
2025-08-07 00:34:57,902 - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
2025-08-07 00:35:01,560 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-07 00:35:05,082 - INFO - Model loaded successfully: 8,030,261,248 parameters
2025-08-07 00:35:05,084 - INFO - Model device: cuda:0
2025-08-07 00:35:05,090 - INFO - Starting training benchmark
2025-08-07 00:35:05,095 - INFO - Pre-tokenizing training data with optimized batches
2025-08-07 00:35:05,172 - INFO - Created 7 batches of size 64
2025-08-07 00:35:05,172 - INFO - Running 25 training batches
2025-08-07 00:35:07,967 - ERROR - Benchmark failed: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 131.00 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 74.54 GiB is allocated by PyTorch, and 3.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
