2025-08-07 01:48:35,365 - INFO - Starting benchmark [PRODUCTION]: meta-llama/Llama-3.1-8B-Instruct on H100
2025-08-07 01:48:35,365 - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
2025-08-07 01:48:38,466 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-07 01:48:41,943 - INFO - Model loaded successfully: 8,030,261,248 parameters
2025-08-07 01:48:41,945 - INFO - Model device: cuda:0
2025-08-07 01:48:41,946 - INFO - Model dtype/quantization: torch.bfloat16
2025-08-07 01:48:41,948 - INFO - Model memory footprint: 14.96 GB
2025-08-07 01:48:41,949 - INFO - Starting inference benchmark
2025-08-07 01:48:41,955 - INFO - Testing inference batch size: 1
2025-08-07 01:48:54,045 - INFO - Iteration 1: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:01,859 - INFO - Iteration 2: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:11,443 - INFO - Iteration 3: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:19,315 - INFO - Iteration 4: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:27,108 - INFO - Iteration 5: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:34,949 - INFO - Iteration 6: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:42,764 - INFO - Iteration 7: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:50,590 - INFO - Iteration 8: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:58,339 - INFO - Iteration 9: Input=7 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=1
2025-08-07 01:49:58,340 - INFO - Batch 1 - Throughput: 7.8 tok/s, Latency: 130.23 ms/tok
2025-08-07 01:49:58,340 - INFO - Testing inference batch size: 4
2025-08-07 01:50:08,667 - INFO - Iteration 1: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:50:18,812 - INFO - Iteration 2: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:50:29,028 - INFO - Iteration 3: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:50:39,149 - INFO - Iteration 4: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:50:49,297 - INFO - Iteration 5: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:51:00,048 - INFO - Iteration 6: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:51:11,578 - INFO - Iteration 7: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:51:21,674 - INFO - Iteration 8: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:51:31,739 - INFO - Iteration 9: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=4
2025-08-07 01:51:31,740 - INFO - Batch 4 - Throughput: 24.8 tok/s, Latency: 40.44 ms/tok
2025-08-07 01:51:31,740 - INFO - Testing inference batch size: 8
2025-08-07 01:51:42,747 - INFO - Iteration 1: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:51:53,463 - INFO - Iteration 2: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:04,198 - INFO - Iteration 3: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:14,913 - INFO - Iteration 4: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:25,675 - INFO - Iteration 5: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:36,455 - INFO - Iteration 6: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:47,202 - INFO - Iteration 7: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:52:58,125 - INFO - Iteration 8: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:53:08,986 - INFO - Iteration 9: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=8
2025-08-07 01:53:08,986 - INFO - Batch 8 - Throughput: 47.5 tok/s, Latency: 21.07 ms/tok
2025-08-07 01:53:08,987 - INFO - Testing inference batch size: 16
2025-08-07 01:53:21,359 - INFO - Iteration 1: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:53:33,578 - INFO - Iteration 2: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:53:45,843 - INFO - Iteration 3: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:53:58,163 - INFO - Iteration 4: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:10,447 - INFO - Iteration 5: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:22,822 - INFO - Iteration 6: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:35,048 - INFO - Iteration 7: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:47,258 - INFO - Iteration 8: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:59,472 - INFO - Iteration 9: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=16
2025-08-07 01:54:59,473 - INFO - Batch 16 - Throughput: 83.6 tok/s, Latency: 11.97 ms/tok
2025-08-07 01:54:59,473 - INFO - Testing inference batch size: 32
2025-08-07 01:55:15,005 - INFO - Iteration 1: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:55:30,426 - INFO - Iteration 2: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:55:45,888 - INFO - Iteration 3: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:56:01,440 - INFO - Iteration 4: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:56:16,942 - INFO - Iteration 5: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:56:32,463 - INFO - Iteration 6: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:56:48,029 - INFO - Iteration 7: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:57:03,564 - INFO - Iteration 8: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:57:19,113 - INFO - Iteration 9: Input=8 tokens, Output=512 tokens, Model dtype=torch.bfloat16, Batch size=32
2025-08-07 01:57:19,114 - INFO - Batch 32 - Throughput: 132.2 tok/s, Latency: 7.56 ms/tok
2025-08-07 01:57:19,182 - INFO - Detailed results: results/raw/H100_meta_llama_Llama_3.1_8B_Instruct_0807_015719.json
2025-08-07 01:57:19,183 - INFO - CSV summary: results/raw/H100_meta_llama_Llama_3.1_8B_Instruct_0807_015719.csv
2025-08-07 01:57:19,185 - INFO - ROI benchmark completed successfully
2025-08-07 01:57:19,186 - INFO - Detailed results: results/raw/H100_meta_llama_Llama_3.1_8B_Instruct_0807_015719.json
2025-08-07 01:57:19,187 - INFO - Summary for spreadsheet: results/raw/H100_meta_llama_Llama_3.1_8B_Instruct_0807_015719.csv
